{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlevqYu5UWuU"
      },
      "source": [
        "# Automatic Speech Recognition with Transformer\n",
        "\n",
        "**Author:** [Apoorv Nandan](https://twitter.com/NandanApoorv)<br>\n",
        "**Date created:** 2021/01/13<br>\n",
        "**Last modified:** 2021/01/13<br>\n",
        "**Description:** Training a sequence-to-sequence Transformer for automatic speech recognition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LtA-e1_UWuX"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Automatic speech recognition (ASR) consists of transcribing audio speech segments into text.\n",
        "ASR can be treated as a sequence-to-sequence problem, where the\n",
        "audio can be represented as a sequence of feature vectors\n",
        "and the text as a sequence of characters, words, or subword tokens.\n",
        "\n",
        "For this demonstration, we will use the LJSpeech dataset from the\n",
        "[LibriVox](https://librivox.org/) project. It consists of short\n",
        "audio clips of a single speaker reading passages from 7 non-fiction books.\n",
        "Our model will be similar to the original Transformer (both encoder and decoder)\n",
        "as proposed in the paper, \"Attention is All You Need\".\n",
        "\n",
        "\n",
        "**References:**\n",
        "\n",
        "- [Attention is All You Need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n",
        "- [Very Deep Self-Attention Networks for End-to-End Speech Recognition](https://arxiv.org/pdf/1904.13377.pdf)\n",
        "- [Speech Transformers](https://ieeexplore.ieee.org/document/8462506)\n",
        "- [LJSpeech Dataset](https://keithito.com/LJ-Speech-Dataset/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQ7TUYyGUWuY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from glob import glob\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuM97_QvUWub"
      },
      "source": [
        "## Define the Transformer Input Layer\n",
        "\n",
        "When processing past target tokens for the decoder, we compute the sum of\n",
        "position embeddings and token embeddings.\n",
        "\n",
        "When processing audio features, we apply convolutional layers to downsample\n",
        "them (via convolution stides) and process local relationships."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-CHG-zeUWuc"
      },
      "outputs": [],
      "source": [
        "class TokenEmbedding(layers.Layer):\n",
        "    def __init__(self, num_vocab=1000, maxlen=100, num_hid=64):\n",
        "        super().__init__()\n",
        "        self.emb = tf.keras.layers.Embedding(num_vocab, num_hid)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        x = self.emb(x)\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        return x + positions\n",
        "\n",
        "\n",
        "class SpeechFeatureEmbedding(layers.Layer):\n",
        "    def __init__(self, num_hid=64, maxlen=100):\n",
        "        super().__init__()\n",
        "        self.conv1 = tf.keras.layers.Conv1D(\n",
        "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
        "        )\n",
        "        self.conv2 = tf.keras.layers.Conv1D(\n",
        "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
        "        )\n",
        "        self.conv3 = tf.keras.layers.Conv1D(\n",
        "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
        "        )\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        return self.conv3(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lP9OahcmUWuc"
      },
      "source": [
        "## Transformer Encoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQElU18vUWud"
      },
      "outputs": [],
      "source": [
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhINEZHrUWud"
      },
      "source": [
        "## Transformer Decoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kr_clUUfUWue"
      },
      "outputs": [],
      "source": [
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.self_att = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.enc_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.self_dropout = layers.Dropout(0.5)\n",
        "        self.enc_dropout = layers.Dropout(0.1)\n",
        "        self.ffn_dropout = layers.Dropout(0.1)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n",
        "        \"\"\"Masks the upper half of the dot product matrix in self attention.\n",
        "\n",
        "        This prevents flow of information from future tokens to current token.\n",
        "        1's in the lower triangle, counting from the lower right corner.\n",
        "        \"\"\"\n",
        "        i = tf.range(n_dest)[:, None]\n",
        "        j = tf.range(n_src)\n",
        "        m = i >= j - n_src + n_dest\n",
        "        mask = tf.cast(m, dtype)\n",
        "        mask = tf.reshape(mask, [1, n_dest, n_src])\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
        "        )\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, enc_out, target):\n",
        "        input_shape = tf.shape(target)\n",
        "        batch_size = input_shape[0]\n",
        "        seq_len = input_shape[1]\n",
        "        causal_mask = self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
        "        target_att = self.self_att(target, target, attention_mask=causal_mask)\n",
        "        target_norm = self.layernorm1(target + self.self_dropout(target_att))\n",
        "        enc_out = self.enc_att(target_norm, enc_out)\n",
        "        enc_out_norm = self.layernorm2(self.enc_dropout(enc_out) + target_norm)\n",
        "        ffn_out = self.ffn(enc_out_norm)\n",
        "        ffn_out_norm = self.layernorm3(enc_out_norm + self.ffn_dropout(ffn_out))\n",
        "        return ffn_out_norm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fm7R5XdEUWue"
      },
      "source": [
        "## Complete the Transformer model\n",
        "\n",
        "Our model takes audio spectrograms as inputs and predicts a sequence of characters.\n",
        "During training, we give the decoder the target character sequence shifted to the left\n",
        "as input. During inference, the decoder uses its own past predictions to predict the\n",
        "next token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nlsMZMBUWue"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Transformer(keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_hid=64,\n",
        "        num_head=2,\n",
        "        num_feed_forward=128,\n",
        "        source_maxlen=100,\n",
        "        target_maxlen=100,\n",
        "        num_layers_enc=4,\n",
        "        num_layers_dec=1,\n",
        "        num_classes=10,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.loss_metric = keras.metrics.Mean(name=\"loss\")\n",
        "        self.num_layers_enc = num_layers_enc\n",
        "        self.num_layers_dec = num_layers_dec\n",
        "        self.target_maxlen = target_maxlen\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.enc_input = SpeechFeatureEmbedding(num_hid=num_hid, maxlen=source_maxlen)\n",
        "        self.dec_input = TokenEmbedding(\n",
        "            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\n",
        "        )\n",
        "\n",
        "        self.encoder = keras.Sequential(\n",
        "            [self.enc_input]\n",
        "            + [\n",
        "                TransformerEncoder(num_hid, num_head, num_feed_forward)\n",
        "                for _ in range(num_layers_enc)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        for i in range(num_layers_dec):\n",
        "            setattr(\n",
        "                self,\n",
        "                f\"dec_layer_{i}\",\n",
        "                TransformerDecoder(num_hid, num_head, num_feed_forward),\n",
        "            )\n",
        "\n",
        "        self.classifier = layers.Dense(num_classes)\n",
        "\n",
        "    def decode(self, enc_out, target):\n",
        "        y = self.dec_input(target)\n",
        "        for i in range(self.num_layers_dec):\n",
        "            y = getattr(self, f\"dec_layer_{i}\")(enc_out, y)\n",
        "        return y\n",
        "\n",
        "    def call(self, inputs):\n",
        "        source = inputs[0]\n",
        "        target = inputs[1]\n",
        "        x = self.encoder(source)\n",
        "        y = self.decode(x, target)\n",
        "        return self.classifier(y)\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_metric]\n",
        "\n",
        "    def train_step(self, batch):\n",
        "        \"\"\"Processes one batch inside model.fit().\"\"\"\n",
        "        source = batch[\"source\"]\n",
        "        target = batch[\"target\"]\n",
        "        dec_input = target[:, :-1]\n",
        "        dec_target = target[:, 1:]\n",
        "        with tf.GradientTape() as tape:\n",
        "            preds = self([source, dec_input])\n",
        "            one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
        "            mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n",
        "            loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "        self.loss_metric.update_state(loss)\n",
        "        return {\"loss\": self.loss_metric.result()}\n",
        "\n",
        "    def test_step(self, batch):\n",
        "        source = batch[\"source\"]\n",
        "        target = batch[\"target\"]\n",
        "        dec_input = target[:, :-1]\n",
        "        dec_target = target[:, 1:]\n",
        "        preds = self([source, dec_input])\n",
        "        one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
        "        mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n",
        "        loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n",
        "        self.loss_metric.update_state(loss)\n",
        "        return {\"loss\": self.loss_metric.result()}\n",
        "\n",
        "    def generate(self, source, target_start_token_idx):\n",
        "        \"\"\"Performs inference over one batch of inputs using greedy decoding.\"\"\"\n",
        "        bs = tf.shape(source)[0]\n",
        "        enc = self.encoder(source)\n",
        "        dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\n",
        "        dec_logits = []\n",
        "        for i in range(self.target_maxlen - 1):\n",
        "            dec_out = self.decode(enc, dec_input)\n",
        "            logits = self.classifier(dec_out)\n",
        "            logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
        "            last_logit = tf.expand_dims(logits[:, -1], axis=-1)\n",
        "            dec_logits.append(last_logit)\n",
        "            dec_input = tf.concat([dec_input, last_logit], axis=-1)\n",
        "        return dec_input\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vw-_2_wNUWuf"
      },
      "source": [
        "## Download the dataset\n",
        "\n",
        "Note: This requires ~3.6 GB of disk space and\n",
        "takes ~5 minutes for the extraction of files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auW1nz4EUWuf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44553d88-a4f0-418a-f6a5-9556a6c41869"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\n",
            "2748572632/2748572632 [==============================] - 77s 0us/step\n"
          ]
        }
      ],
      "source": [
        "keras.utils.get_file(\n",
        "    os.path.join(os.getcwd(), \"data.tar.gz\"),\n",
        "    \"https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\",\n",
        "    extract=True,\n",
        "    archive_format=\"tar\",\n",
        "    cache_dir=\".\",\n",
        ")\n",
        "\n",
        "\n",
        "saveto = \"./datasets/LJSpeech-1.1\"\n",
        "wavs = glob(\"{}/**/*.wav\".format(saveto), recursive=True)\n",
        "\n",
        "id_to_text = {}\n",
        "with open(os.path.join(saveto, \"metadata.csv\"), encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        id = line.strip().split(\"|\")[0]\n",
        "        text = line.strip().split(\"|\")[2]\n",
        "        id_to_text[id] = text\n",
        "\n",
        "\n",
        "def get_data(wavs, id_to_text, maxlen=50):\n",
        "    \"\"\" returns mapping of audio paths and transcription texts \"\"\"\n",
        "    data = []\n",
        "    for w in wavs:\n",
        "        id = w.split(\"/\")[-1].split(\".\")[0]\n",
        "        if len(id_to_text[id]) < maxlen:\n",
        "            data.append({\"audio\": w, \"text\": id_to_text[id]})\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4gpi7TGUWug"
      },
      "source": [
        "## Preprocess the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxnXaCFGUWug",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9856bf63-73f4-44b3-c043-33eb4c4dde49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size 34\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class VectorizeChar:\n",
        "    def __init__(self, max_len=50):\n",
        "        self.vocab = (\n",
        "            [\"-\", \"#\", \"<\", \">\"]\n",
        "            + [chr(i + 96) for i in range(1, 27)]\n",
        "            + [\" \", \".\", \",\", \"?\"]\n",
        "        )\n",
        "        self.max_len = max_len\n",
        "        self.char_to_idx = {}\n",
        "        for i, ch in enumerate(self.vocab):\n",
        "            self.char_to_idx[ch] = i\n",
        "\n",
        "    def __call__(self, text):\n",
        "        text = text.lower()\n",
        "        text = text[: self.max_len - 2]\n",
        "        text = \"<\" + text + \">\"\n",
        "        pad_len = self.max_len - len(text)\n",
        "        return [self.char_to_idx.get(ch, 1) for ch in text] + [0] * pad_len\n",
        "\n",
        "    def get_vocabulary(self):\n",
        "        return self.vocab\n",
        "\n",
        "\n",
        "max_target_len = 200  # all transcripts in out data are < 200 characters\n",
        "data = get_data(wavs, id_to_text, max_target_len)\n",
        "vectorizer = VectorizeChar(max_target_len)\n",
        "print(\"vocab size\", len(vectorizer.get_vocabulary()))\n",
        "\n",
        "\n",
        "def create_text_ds(data):\n",
        "    texts = [_[\"text\"] for _ in data]\n",
        "    text_ds = [vectorizer(t) for t in texts]\n",
        "    text_ds = tf.data.Dataset.from_tensor_slices(text_ds)\n",
        "    return text_ds\n",
        "\n",
        "\n",
        "def path_to_audio(path):\n",
        "    # spectrogram using stft\n",
        "    audio = tf.io.read_file(path)\n",
        "    audio, _ = tf.audio.decode_wav(audio, 1)\n",
        "    audio = tf.squeeze(audio, axis=-1)\n",
        "    stfts = tf.signal.stft(audio, frame_length=200, frame_step=80, fft_length=256)\n",
        "    x = tf.math.pow(tf.abs(stfts), 0.5)\n",
        "    # normalisation\n",
        "    means = tf.math.reduce_mean(x, 1, keepdims=True)\n",
        "    stddevs = tf.math.reduce_std(x, 1, keepdims=True)\n",
        "    x = (x - means) / stddevs\n",
        "    audio_len = tf.shape(x)[0]\n",
        "    # padding to 10 seconds\n",
        "    pad_len = 2754\n",
        "    paddings = tf.constant([[0, pad_len], [0, 0]])\n",
        "    x = tf.pad(x, paddings, \"CONSTANT\")[:pad_len, :]\n",
        "    return x\n",
        "\n",
        "\n",
        "def create_audio_ds(data):\n",
        "    flist = [_[\"audio\"] for _ in data]\n",
        "    audio_ds = tf.data.Dataset.from_tensor_slices(flist)\n",
        "    audio_ds = audio_ds.map(\n",
        "        path_to_audio, num_parallel_calls=tf.data.AUTOTUNE\n",
        "    )\n",
        "    return audio_ds\n",
        "\n",
        "\n",
        "def create_tf_dataset(data, bs=4):\n",
        "    audio_ds = create_audio_ds(data)\n",
        "    text_ds = create_text_ds(data)\n",
        "    ds = tf.data.Dataset.zip((audio_ds, text_ds))\n",
        "    ds = ds.map(lambda x, y: {\"source\": x, \"target\": y})\n",
        "    ds = ds.batch(bs)\n",
        "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "\n",
        "split = int(len(data) * 0.99)\n",
        "train_data = data[:split]\n",
        "test_data = data[split:]\n",
        "ds = create_tf_dataset(train_data, bs=64)\n",
        "val_ds = create_tf_dataset(test_data, bs=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9tl25QiUWug"
      },
      "source": [
        "## Callbacks to display predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0yS3_KcUWug"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DisplayOutputs(keras.callbacks.Callback):\n",
        "    def __init__(\n",
        "        self, batch, idx_to_token, target_start_token_idx=27, target_end_token_idx=28\n",
        "    ):\n",
        "        \"\"\"Displays a batch of outputs after every epoch\n",
        "\n",
        "        Args:\n",
        "            batch: A test batch containing the keys \"source\" and \"target\"\n",
        "            idx_to_token: A List containing the vocabulary tokens corresponding to their indices\n",
        "            target_start_token_idx: A start token index in the target vocabulary\n",
        "            target_end_token_idx: An end token index in the target vocabulary\n",
        "        \"\"\"\n",
        "        self.batch = batch\n",
        "        self.target_start_token_idx = target_start_token_idx\n",
        "        self.target_end_token_idx = target_end_token_idx\n",
        "        self.idx_to_char = idx_to_token\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if epoch % 5 != 0:\n",
        "            return\n",
        "        source = self.batch[\"source\"]\n",
        "        target = self.batch[\"target\"].numpy()\n",
        "        bs = tf.shape(source)[0]\n",
        "        preds = self.model.generate(source, self.target_start_token_idx)\n",
        "        preds = preds.numpy()\n",
        "        for i in range(bs):\n",
        "            target_text = \"\".join([self.idx_to_char[_] for _ in target[i, :]])\n",
        "            prediction = \"\"\n",
        "            for idx in preds[i, :]:\n",
        "                prediction += self.idx_to_char[idx]\n",
        "                if idx == self.target_end_token_idx:\n",
        "                    break\n",
        "            print(f\"target:     {target_text.replace('-','')}\")\n",
        "            print(f\"prediction: {prediction}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6WqKw_fUWuh"
      },
      "source": [
        "## Learning rate schedule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJVTRpceUWuh"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CustomSchedule(keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        init_lr=0.00001,\n",
        "        lr_after_warmup=0.001,\n",
        "        final_lr=0.00001,\n",
        "        warmup_epochs=15,\n",
        "        decay_epochs=85,\n",
        "        steps_per_epoch=203,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.init_lr = init_lr\n",
        "        self.lr_after_warmup = lr_after_warmup\n",
        "        self.final_lr = final_lr\n",
        "        self.warmup_epochs = warmup_epochs\n",
        "        self.decay_epochs = decay_epochs\n",
        "        self.steps_per_epoch = steps_per_epoch\n",
        "\n",
        "    def calculate_lr(self, epoch):\n",
        "        \"\"\" linear warm up - linear decay \"\"\"\n",
        "        warmup_lr = (\n",
        "            self.init_lr\n",
        "            + ((self.lr_after_warmup - self.init_lr) / (self.warmup_epochs - 1)) * epoch\n",
        "        )\n",
        "        decay_lr = tf.math.maximum(\n",
        "            self.final_lr,\n",
        "            self.lr_after_warmup\n",
        "            - (epoch - self.warmup_epochs)\n",
        "            * (self.lr_after_warmup - self.final_lr)\n",
        "            / (self.decay_epochs),\n",
        "        )\n",
        "        return tf.math.minimum(warmup_lr, decay_lr)\n",
        "\n",
        "    def __call__(self, step):\n",
        "        epoch = step // self.steps_per_epoch\n",
        "        return self.calculate_lr(epoch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o18XRgsGUWuh"
      },
      "source": [
        "## Create & train the end-to-end model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rE_IzJbQUWuh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7bf52ba-2c29-4461-cf75-cdc52a71ba2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "203/203 [==============================] - ETA: 0s - loss: 1.7559target:     <the neglect of the condemned convicts, the filthy condition of the wards, the insufficiency of bedding and clothing,>\n",
            "prediction: <t t t t t the  tho t tt e tt ae athe te e an the t te t t ine oe t t t te ae t ind t ta t ate t  te an t t t tt t atbe t t ae antse int a ae  at t ans the w t att ore e t he he tinme ant tat e a a i \n",
            "\n",
            "target:     <it bore the manufacturer#s serial number c two seven six six.>\n",
            "prediction: <t t t t t the  tho t tthe tt ae athe te e an the t te t t ine oe t t t te ae t ind t ta t ate t  te an t t t tt t atbe t t ae antse int a ae  at t ans the w t att ore e t he he tinme ant tat e a a i \n",
            "\n",
            "target:     <last thursday i described the american form of government as a three horse team provided by the constitution to the american people>\n",
            "prediction: <t t t t t the  tho t tt e tt ae athe te e an the t te t t ine oe t t t te ae t ind t ta t ate t  te an t t t tt t atbe t t ae antse int a ae  at t ans the w t att ore e t he he tinme ant tat e a a i \n",
            "\n",
            "target:     <in one prison the bedsteads had been removed lest the prisoners should break them up and convert them into weapons of offense.>\n",
            "prediction: <t t t t t the  tho t tt e tt ae athe te e an the t te t t ine oe t t t te ae t ind t ta t ate t  te an t t t tt t atbe t t ae antse int a ae  at t ans the w t att ore e t he he tinme ant tat e a a i \n",
            "\n",
            "203/203 [==============================] - 201s 959ms/step - loss: 1.7559 - val_loss: 1.4580\n",
            "Epoch 2/20\n",
            "203/203 [==============================] - 184s 906ms/step - loss: 1.4006 - val_loss: 1.2497\n",
            "Epoch 3/20\n",
            "203/203 [==============================] - 184s 903ms/step - loss: 1.3236 - val_loss: 1.2228\n",
            "Epoch 4/20\n",
            "203/203 [==============================] - 185s 908ms/step - loss: 1.3078 - val_loss: 1.2164\n",
            "Epoch 5/20\n",
            "203/203 [==============================] - 185s 910ms/step - loss: 1.2981 - val_loss: 1.2061\n",
            "Epoch 6/20\n",
            "203/203 [==============================] - ETA: 0s - loss: 1.2806target:     <the neglect of the condemned convicts, the filthy condition of the wards, the insufficiency of bedding and clothing,>\n",
            "prediction: <the the the the f the the f the the the f the the ff the the foff the the the the the the the f the the the ff the the the the the the the the the athe.>\n",
            "\n",
            "target:     <it bore the manufacturer#s serial number c two seven six six.>\n",
            "prediction: <in the the the the the the the the the s the the the the the se the the se an there the the therere.>\n",
            "\n",
            "target:     <last thursday i described the american form of government as a three horse team provided by the constitution to the american people>\n",
            "prediction: <whe the the the the the the the the as the the as the as the as f the the the the an an an the the f the the the the the an the are the the the the the pre the toff therinde kennedy.>\n",
            "\n",
            "target:     <in one prison the bedsteads had been removed lest the prisoners should break them up and convert them into weapons of offense.>\n",
            "prediction: <in the the the the the the the the the the the the the the the the the the pre the the the the the f the the re the re re the the the ss.>\n",
            "\n",
            "203/203 [==============================] - 188s 924ms/step - loss: 1.2806 - val_loss: 1.1768\n",
            "Epoch 7/20\n",
            "203/203 [==============================] - 183s 900ms/step - loss: 1.2394 - val_loss: 1.1196\n",
            "Epoch 8/20\n",
            "203/203 [==============================] - 183s 900ms/step - loss: 1.1508 - val_loss: 1.0035\n",
            "Epoch 9/20\n",
            "203/203 [==============================] - 182s 893ms/step - loss: 1.0015 - val_loss: 0.8544\n",
            "Epoch 10/20\n",
            "203/203 [==============================] - 183s 897ms/step - loss: 0.8652 - val_loss: 0.7597\n",
            "Epoch 11/20\n",
            "203/203 [==============================] - ETA: 0s - loss: 0.7804target:     <the neglect of the condemned convicts, the filthy condition of the wards, the insufficiency of bedding and clothing,>\n",
            "prediction: <the necondition of the filficiention of the condition of the filficiention of the condition of the wards the a vewards the ong.>\n",
            "\n",
            "target:     <it bore the manufacturer#s serial number c two seven six six.>\n",
            "prediction: <it porthe many factures.>\n",
            "\n",
            "target:     <last thursday i described the american form of government as a three horse team provided by the constitution to the american people>\n",
            "prediction: <lats the for i the horsed ay the constitution of govers deay the americant as the amerst the americant as the amerst the as and anthe asthe an ay ay an an ay on of anthedent kennery.>\n",
            "\n",
            "target:     <in one prison the bedsteads had been removed lest the prisoners should break them up and convert them into weapons of offense.>\n",
            "prediction: <in verison one prison one prison one prison a fentsence had brison a fents the prison, and remove fents.>\n",
            "\n",
            "203/203 [==============================] - 186s 915ms/step - loss: 0.7804 - val_loss: 0.7122\n",
            "Epoch 12/20\n",
            "203/203 [==============================] - 182s 893ms/step - loss: 0.7290 - val_loss: 0.6872\n",
            "Epoch 13/20\n",
            "203/203 [==============================] - 181s 891ms/step - loss: 0.6940 - val_loss: 0.6610\n",
            "Epoch 14/20\n",
            "203/203 [==============================] - 181s 889ms/step - loss: 0.6686 - val_loss: 0.6402\n",
            "Epoch 15/20\n",
            "203/203 [==============================] - 182s 895ms/step - loss: 0.6436 - val_loss: 0.6318\n",
            "Epoch 16/20\n",
            "203/203 [==============================] - ETA: 0s - loss: 0.6242target:     <the neglect of the condemned convicts, the filthy condition of the wards, the insufficiency of bedding and clothing,>\n",
            "prediction: <the nit lecondition of the wards the fill the condition sufficiention of the wards the fill the condition of the condition.>\n",
            "\n",
            "target:     <it bore the manufacturer#s serial number c two seven six six.>\n",
            "prediction: <it porthe man factures seven sixerial the many factures.>\n",
            "\n",
            "target:     <last thursday i described the american form of government as a three horse team provided by the constitution to the american people>\n",
            "prediction: <lats ried be scribed be scribed be the cried be the cried be provided be the cried be the constitution the emere of governmenthe amenthe theen the anthentorioriont antentent by a.>\n",
            "\n",
            "target:     <in one prison the bedsteads had been removed lest the prisoners should break them up and convert them into weapons of offense.>\n",
            "prediction: <in vert them into weapon prison the prisoners of fensted them into weap and conveds hould been to weap and conveds hould brithe thesonthes.>\n",
            "\n",
            "203/203 [==============================] - 185s 910ms/step - loss: 0.6242 - val_loss: 0.6199\n",
            "Epoch 17/20\n",
            "203/203 [==============================] - 183s 898ms/step - loss: 0.6059 - val_loss: 0.6091\n",
            "Epoch 18/20\n",
            "203/203 [==============================] - 185s 909ms/step - loss: 0.5888 - val_loss: 0.5956\n",
            "Epoch 19/20\n",
            "203/203 [==============================] - 186s 914ms/step - loss: 0.5742 - val_loss: 0.5900\n",
            "Epoch 20/20\n",
            "203/203 [==============================] - 186s 914ms/step - loss: 0.5616 - val_loss: 0.5905\n"
          ]
        }
      ],
      "source": [
        "batch = next(iter(val_ds))\n",
        "\n",
        "# The vocabulary to convert predicted indices into characters\n",
        "idx_to_char = vectorizer.get_vocabulary()\n",
        "display_cb = DisplayOutputs(\n",
        "    batch, idx_to_char, target_start_token_idx=2, target_end_token_idx=3)\n",
        "\n",
        "# set the arguments as per vocabulary index for '<' and '>'\n",
        "model = Transformer(\n",
        "    num_hid=200,\n",
        "    num_head=2,\n",
        "    num_feed_forward=400,\n",
        "    target_maxlen=max_target_len,\n",
        "    num_layers_enc=4,\n",
        "    num_layers_dec=1,\n",
        "    num_classes=34,\n",
        ")\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy(\n",
        "    from_logits=True, label_smoothing=0.1,\n",
        ")\n",
        "\n",
        "learning_rate = CustomSchedule(\n",
        "    init_lr=0.00001,\n",
        "    lr_after_warmup=0.001,\n",
        "    final_lr=0.00001,\n",
        "    warmup_epochs=15,\n",
        "    decay_epochs=85,\n",
        "    steps_per_epoch=len(ds),\n",
        ")\n",
        "optimizer = keras.optimizers.Adam(learning_rate)\n",
        "model.compile(optimizer=optimizer, loss=loss_fn)\n",
        "\n",
        "history = model.fit(ds, validation_data=val_ds, callbacks=[display_cb], epochs=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlnNAFbRUWuh"
      },
      "source": [
        "In practice, you should train for around 100 epochs or more.\n",
        "\n",
        "Some of the predicted text at or around epoch 35 may look as follows:\n",
        "```\n",
        "target:     <as they sat in the car, frazier asked oswald where his lunch was>\n",
        "prediction: <as they sat in the car frazier his lunch ware mis lunch was>\n",
        "\n",
        "target:     <under the entry for may one, nineteen sixty,>\n",
        "prediction: <under the introus for may monee, nin the sixty,>\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}